{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to ISAC-SIMO Documentation Introduction Intelligent Supervision Assistant for Construction - Sistema Inteligente de Monitoreo de Obra ISAC-SIMO is a system to validate that the intervention work done for homeowners has been done correctly and safely. It is a Build Change project supported by a grant from IBM. Project Detail The technology consists of a mobile application in order to track the progression of an intervention on a home throughout the process to complete work. The application can validate and analyze the quality of building elements, rebar, walls etc. by guiding the users through a series of checks. In addition to the mobile application, the tool also consists of a web interface that facilitates the management of checks and image processing pipelines implemented. Overview of the ISAC-SIMO tool Download as PDF View YouTube Tutorial","title":"Home"},{"location":"#welcome-to-isac-simo-documentation","text":"","title":"Welcome to ISAC-SIMO Documentation"},{"location":"#introduction","text":"Intelligent Supervision Assistant for Construction - Sistema Inteligente de Monitoreo de Obra ISAC-SIMO is a system to validate that the intervention work done for homeowners has been done correctly and safely. It is a Build Change project supported by a grant from IBM.","title":"Introduction"},{"location":"#project-detail","text":"The technology consists of a mobile application in order to track the progression of an intervention on a home throughout the process to complete work. The application can validate and analyze the quality of building elements, rebar, walls etc. by guiding the users through a series of checks. In addition to the mobile application, the tool also consists of a web interface that facilitates the management of checks and image processing pipelines implemented. Overview of the ISAC-SIMO tool Download as PDF View YouTube Tutorial","title":"Project Detail"},{"location":"contribute/","text":"Call to Action for Developers We would like to invite developers and other interested folks to contribute to further development of this project and help make ISAC-SIMO a robust tool with a wide catalog of quality checks accessible by homeowners, builders, and local authorities to enable safe construction practices in areas with lack of technical support. The potential areas for further developments are listed below. Short term updates: User interface improvements: Suggest improvements to the UI of the mobile app to make the process as intuitive and user-friendly for non-technical users. Suggest improvements or new features to the dashboard to enable a wider use of the platform and cater to different users like developers, project managers, general users etc. Crowdsource image dataset for ML training: Contribute image dataset for different construction elements (eg. walls and type of walls, openings, rebar cages, rebar stirrups etc.) that can be used to train object detection or segmentation models to detect and extract key construction elements from a construction site image. To contribute image dataset of construction elements, both ISAC-SIMO mobile app or the dashboard can be used by following these guidelines . Auto-perspective fix of wall / facade images: Support to automatically detect a skewed perspective in an image, and automatically fix the perspective of the image to front perspective. To detect the perspective, the segmentation mask obtained after passing the raw image through the unet model can be used instead of the raw image to make it easier to detect a skewed perspective in an image. Figure : Top row - Images that need a perspective fix Bottom row - Images that don\u2019t need a perspective fix Figure : Segmentation mask obtained from trained Unet model Top row - Images needing perspective fix Bottom row - Images that don\u2019t need perspective fix To download these images for trial, click here . Long term updates: In the long term, we envision ISAC-SIMO to contain a wide catalogue of checks that can be deployed in multiple contexts around the world to help bridge the gap in technical support to homeowners, builders, and local authorities to assist with the construction of disaster resilient confined masonry houses. To create a seamless experience for the users, we would need to be able to detect key construction elements from photos of construction sites taken at different stages of construction, and assess the detected elements for compliance or non-compliance. In order to achieve that, we would need to: 1) train a wide variety of models that can identify and extract key construction elements from construction site images, and 2) classify or process the images to assess the quality of the identified construction element as per the recommended guideline. We would like to invite interested developers and other supporters of this project to contribute to the development of more quality checks in the long term with the following activities: Crowd-source image dataset to train new models and create new checks Train object detection models to identify key construction components from construction site images Train new machine learning models or contribute python scripts to help extract key features in the images of construction elements and assess their quality as per the recommended guidelines To implement the checks, we can use a combination of machine learning models (such as object detection, segmentation, classification models) along with python scripts to carry out image processing and compute the final result of an assessment. We can thereby implement a three-step pipeline in the backend for each check: 1) Object Detection: Implement an object detection model to detect the construction element of interest from an image of a construction site. 2) Pre-processing: Implement a pre-processing python script to extract the bounding box corresponding to the detected construction element and pre-process the image using a pre-trained deep learning model or image processing functions, as appropriate for the check. 3) Post-processing: Implement a post-processing python script to analyze the segmentation mask or the processed image to extract key features and compute compliance or non-compliance as per the check requirements using machine learning models or python. The list of construction elements to be detected and the catalogue of quality checks that can be implemented for confined masonry houses are detailed in the next section. Object Detection Given enough image dataset, we can train and deploy object detection models to identify key construction elements from construction site images. The key components of a confined masonry construction is shown in the figure below. Figure: Key components and characteristics of a confined masonry building (Schacher 2015) Out of the confined masonry building components, we can train object detection models to identify the elements listed in the table below. # Element to be detected Example(s) Description 1 Facade Detect a facade of a building 2 Storeys Detect multiple storeys in a facade of a building 3 Openings Detect openings and their position 4 Masonry Walls Detect full solid wall panels 5 Confining Concrete Elements Naming of horizontal and vertical ties Detect confining elements such as tie beams and tie columns 6 Rebar Elements Detect rebar elements such as rebar cage, rebar stirrup, longitudinal rebars, and seismic bands and obtain segmentation masks for rebar cages. Note: It may not be necessary to create separate colored masks for horizontal and vertical rebar elements as shown in the example and might suffice to represent all rebar with the same color. Table 1: Key construction elements that can be detected for further assessment of confined masonry building Quality Checks After detecting the key construction elements, we can extract the region corresponding to the element detected along with the position of the element in the original image for further analysis of compliance. Depending on the need of each quality check, we can either process the image to extract key features using python scripts to compute the compliance, or deploy additional machine learning models to detect key features before determining the final result. The catalogue of checks that can be developed for a variety of confined masonry house typologies are listed in the table below. Although in reality there are more checks that can be developed for confined masonry houses, a lot of the checks depend on the capability to extract accurate measurements to determine the compliance. So the following list outlines the checks that can be performed using relative comparisons without necessarily getting the exact measurements. # Check Example Description & Applicability 1 Masonry unit type (Hollow clay block, Hollow concrete block, solid clay block, solid concrete block, perforated clay block, etc.) Detect the type of masonry unit used for construction. Although a variety of masonry units are used in the construction of confined masonry buildings, the type of block used determines the overall strength of the masonry walls and thereby have different requirement for compliance 2 Percentage of holes in hollow blocks For hollow or perforated masonry units, check for ratio of holes to horizontal surface 3 Type of Masonry Wall Classify the type of wall. Eg: Plastered wall, concrete block wall, clay block wall etc. 4 Bond pattern check for concrete blocks A quality check to assess the bond pattern in concrete block walls. Note: This check has been created so far for the clay block walls by using a combination of segmentation step using unet model followed by image processing of the mask image. A similar approach can be used for the concrete block wall, but we would need a different model to detect the mortar regions in the image. 5 Mortar thickness check for concrete blocks A quality check to assess the mortar thickness in concrete block walls 6 Wall thickness A quality check to compare the thickness of the wall to the total height of the wall. Note: The exact requirement for the ratio of wall thickness to height may vary depending on the type of block used to build the wall 7 Toothing in wall column intersections Detect if toothing is present at the wall column intersections, and if so, determine if it\u2019s less than \u00bd of brick length 8 Opening Size After detecting walls with openings, calculate the relative width of the opening with the length of the wall panel 9 Detect shear walls on each facade (wall panels without openings that are confined) Detect if the facade has at least one wall panel that is confined with tie columns 10 Vertical continuity of openings (in case of multi-storey house) Check for continuity of openings in multi-storey buildings. 11 Presence or absence of confining elements around openings Openings with confinement Detect presence of confining elements around openings 12 Rebar Cage Quality and Stirrup Spacing Actual rebar cage requirements Detect poor quality reused rebar by identifying features such as bent longitudinal rebars, rebar cages with rebar stirrups that are too far apart. We can compute the no. of rebar ties relative to the height of the longitudinal rebar, and the spacing between rebar ties with the help of their position, in order to approximately assess if the stirrups are too far apart. Note: In order to carry out this analysis, we might need to first obtain a segmentation mask of horizontal and vertical rebar elements Parameters for approximate assessment Note: This is not a comprehensive list of checks but includes checks that can be done visually. Table 2: Quality checks that can be developed for confined masonry buildings Mobile App Updates Additional functionalities that could enhance the performance and improve applicability of the tool in future implementations. Support for streaming and video processing Implement AR for providing estimated measurements Add more features in the mobile app: Offline functionality Third Party Integration Multiple language support Auto-object detection of construction elements Detects Objects when capturing images without having to choose the check type manually. Real time object detection when capturing picture Detects all objects (wall, rebar, brick etc.) in the camera screen in real-time and shows squares around them. And Be able to toggle on/off. Quality Check via Video Recording Support the Quality Check to be performed by recording video of the construction site. Instead of only Photos, we can also allow recording videos. The Back-end API exists but might need optimization. Store and Review previous test results For Guest as well as Logged in users, add a feature to store previous tests performed in that device to review later. Currently we only see once. Web Platform Updates More Model Format Support: Currently ISAC-SIMO supports h5, hdf5, keras and standard python3 scripts. It can be upgraded to support Pickled Python (pkl), Petastorm, Protobuf (pb), Apple ML Model (mlmodel), Torch Script (pt) and other popular machine learning file stores and formats. Support Multiple Image URLs: When testing photos against checks, users can either upload multiple image files or provide image url (currently only single). We can add features to support multiple image urls and store them in a single row instead of having a single image url per row in test result. As we have to fetch multiple images from multiple image urls, it might cause an asynchronous problem. Edit Python3 Scripts / Offline Model directly from Web Application: Everytime we need to make some changes to our python3 scripts, pre & post processor we have to edit the offline model and upload a new file every single time. We can have a feature to securely edit Python Scripts directly from the web application and immediately apply the changes. Rate Limiting Per Action: Currently, rate limiting can be handled by web servers (Nginx, Apache). We can have a feature to rate limit users based on logged-in user ID or IP Address in our application. The Rate Limit can be applied separately for separate features / modules. For Example, we can rate limit crowdsource upload per minute, but keep the rate-limit separately for other parts of the application. Captcha Integration: We can integrate Captcha for different actions within our web and mobile applications. We can add captcha verification for registration, running image tests, crowdsource upload, contribution submission etc.","title":"Contribute"},{"location":"contribute/#call-to-action-for-developers","text":"We would like to invite developers and other interested folks to contribute to further development of this project and help make ISAC-SIMO a robust tool with a wide catalog of quality checks accessible by homeowners, builders, and local authorities to enable safe construction practices in areas with lack of technical support. The potential areas for further developments are listed below.","title":"Call to Action for Developers"},{"location":"contribute/#short-term-updates","text":"User interface improvements: Suggest improvements to the UI of the mobile app to make the process as intuitive and user-friendly for non-technical users. Suggest improvements or new features to the dashboard to enable a wider use of the platform and cater to different users like developers, project managers, general users etc. Crowdsource image dataset for ML training: Contribute image dataset for different construction elements (eg. walls and type of walls, openings, rebar cages, rebar stirrups etc.) that can be used to train object detection or segmentation models to detect and extract key construction elements from a construction site image. To contribute image dataset of construction elements, both ISAC-SIMO mobile app or the dashboard can be used by following these guidelines . Auto-perspective fix of wall / facade images: Support to automatically detect a skewed perspective in an image, and automatically fix the perspective of the image to front perspective. To detect the perspective, the segmentation mask obtained after passing the raw image through the unet model can be used instead of the raw image to make it easier to detect a skewed perspective in an image. Figure : Top row - Images that need a perspective fix Bottom row - Images that don\u2019t need a perspective fix Figure : Segmentation mask obtained from trained Unet model Top row - Images needing perspective fix Bottom row - Images that don\u2019t need perspective fix To download these images for trial, click here .","title":"Short term updates:"},{"location":"contribute/#long-term-updates","text":"In the long term, we envision ISAC-SIMO to contain a wide catalogue of checks that can be deployed in multiple contexts around the world to help bridge the gap in technical support to homeowners, builders, and local authorities to assist with the construction of disaster resilient confined masonry houses. To create a seamless experience for the users, we would need to be able to detect key construction elements from photos of construction sites taken at different stages of construction, and assess the detected elements for compliance or non-compliance. In order to achieve that, we would need to: 1) train a wide variety of models that can identify and extract key construction elements from construction site images, and 2) classify or process the images to assess the quality of the identified construction element as per the recommended guideline. We would like to invite interested developers and other supporters of this project to contribute to the development of more quality checks in the long term with the following activities: Crowd-source image dataset to train new models and create new checks Train object detection models to identify key construction components from construction site images Train new machine learning models or contribute python scripts to help extract key features in the images of construction elements and assess their quality as per the recommended guidelines To implement the checks, we can use a combination of machine learning models (such as object detection, segmentation, classification models) along with python scripts to carry out image processing and compute the final result of an assessment. We can thereby implement a three-step pipeline in the backend for each check: 1) Object Detection: Implement an object detection model to detect the construction element of interest from an image of a construction site. 2) Pre-processing: Implement a pre-processing python script to extract the bounding box corresponding to the detected construction element and pre-process the image using a pre-trained deep learning model or image processing functions, as appropriate for the check. 3) Post-processing: Implement a post-processing python script to analyze the segmentation mask or the processed image to extract key features and compute compliance or non-compliance as per the check requirements using machine learning models or python. The list of construction elements to be detected and the catalogue of quality checks that can be implemented for confined masonry houses are detailed in the next section.","title":"Long term updates:"},{"location":"contribute/#object-detection","text":"Given enough image dataset, we can train and deploy object detection models to identify key construction elements from construction site images. The key components of a confined masonry construction is shown in the figure below. Figure: Key components and characteristics of a confined masonry building (Schacher 2015) Out of the confined masonry building components, we can train object detection models to identify the elements listed in the table below. # Element to be detected Example(s) Description 1 Facade Detect a facade of a building 2 Storeys Detect multiple storeys in a facade of a building 3 Openings Detect openings and their position 4 Masonry Walls Detect full solid wall panels 5 Confining Concrete Elements Naming of horizontal and vertical ties Detect confining elements such as tie beams and tie columns 6 Rebar Elements Detect rebar elements such as rebar cage, rebar stirrup, longitudinal rebars, and seismic bands and obtain segmentation masks for rebar cages. Note: It may not be necessary to create separate colored masks for horizontal and vertical rebar elements as shown in the example and might suffice to represent all rebar with the same color. Table 1: Key construction elements that can be detected for further assessment of confined masonry building","title":"Object Detection"},{"location":"contribute/#quality-checks","text":"After detecting the key construction elements, we can extract the region corresponding to the element detected along with the position of the element in the original image for further analysis of compliance. Depending on the need of each quality check, we can either process the image to extract key features using python scripts to compute the compliance, or deploy additional machine learning models to detect key features before determining the final result. The catalogue of checks that can be developed for a variety of confined masonry house typologies are listed in the table below. Although in reality there are more checks that can be developed for confined masonry houses, a lot of the checks depend on the capability to extract accurate measurements to determine the compliance. So the following list outlines the checks that can be performed using relative comparisons without necessarily getting the exact measurements. # Check Example Description & Applicability 1 Masonry unit type (Hollow clay block, Hollow concrete block, solid clay block, solid concrete block, perforated clay block, etc.) Detect the type of masonry unit used for construction. Although a variety of masonry units are used in the construction of confined masonry buildings, the type of block used determines the overall strength of the masonry walls and thereby have different requirement for compliance 2 Percentage of holes in hollow blocks For hollow or perforated masonry units, check for ratio of holes to horizontal surface 3 Type of Masonry Wall Classify the type of wall. Eg: Plastered wall, concrete block wall, clay block wall etc. 4 Bond pattern check for concrete blocks A quality check to assess the bond pattern in concrete block walls. Note: This check has been created so far for the clay block walls by using a combination of segmentation step using unet model followed by image processing of the mask image. A similar approach can be used for the concrete block wall, but we would need a different model to detect the mortar regions in the image. 5 Mortar thickness check for concrete blocks A quality check to assess the mortar thickness in concrete block walls 6 Wall thickness A quality check to compare the thickness of the wall to the total height of the wall. Note: The exact requirement for the ratio of wall thickness to height may vary depending on the type of block used to build the wall 7 Toothing in wall column intersections Detect if toothing is present at the wall column intersections, and if so, determine if it\u2019s less than \u00bd of brick length 8 Opening Size After detecting walls with openings, calculate the relative width of the opening with the length of the wall panel 9 Detect shear walls on each facade (wall panels without openings that are confined) Detect if the facade has at least one wall panel that is confined with tie columns 10 Vertical continuity of openings (in case of multi-storey house) Check for continuity of openings in multi-storey buildings. 11 Presence or absence of confining elements around openings Openings with confinement Detect presence of confining elements around openings 12 Rebar Cage Quality and Stirrup Spacing Actual rebar cage requirements Detect poor quality reused rebar by identifying features such as bent longitudinal rebars, rebar cages with rebar stirrups that are too far apart. We can compute the no. of rebar ties relative to the height of the longitudinal rebar, and the spacing between rebar ties with the help of their position, in order to approximately assess if the stirrups are too far apart. Note: In order to carry out this analysis, we might need to first obtain a segmentation mask of horizontal and vertical rebar elements Parameters for approximate assessment Note: This is not a comprehensive list of checks but includes checks that can be done visually. Table 2: Quality checks that can be developed for confined masonry buildings","title":"Quality Checks"},{"location":"contribute/#mobile-app-updates","text":"Additional functionalities that could enhance the performance and improve applicability of the tool in future implementations. Support for streaming and video processing Implement AR for providing estimated measurements Add more features in the mobile app: Offline functionality Third Party Integration Multiple language support Auto-object detection of construction elements Detects Objects when capturing images without having to choose the check type manually. Real time object detection when capturing picture Detects all objects (wall, rebar, brick etc.) in the camera screen in real-time and shows squares around them. And Be able to toggle on/off. Quality Check via Video Recording Support the Quality Check to be performed by recording video of the construction site. Instead of only Photos, we can also allow recording videos. The Back-end API exists but might need optimization. Store and Review previous test results For Guest as well as Logged in users, add a feature to store previous tests performed in that device to review later. Currently we only see once.","title":"Mobile App Updates"},{"location":"contribute/#web-platform-updates","text":"More Model Format Support: Currently ISAC-SIMO supports h5, hdf5, keras and standard python3 scripts. It can be upgraded to support Pickled Python (pkl), Petastorm, Protobuf (pb), Apple ML Model (mlmodel), Torch Script (pt) and other popular machine learning file stores and formats. Support Multiple Image URLs: When testing photos against checks, users can either upload multiple image files or provide image url (currently only single). We can add features to support multiple image urls and store them in a single row instead of having a single image url per row in test result. As we have to fetch multiple images from multiple image urls, it might cause an asynchronous problem. Edit Python3 Scripts / Offline Model directly from Web Application: Everytime we need to make some changes to our python3 scripts, pre & post processor we have to edit the offline model and upload a new file every single time. We can have a feature to securely edit Python Scripts directly from the web application and immediately apply the changes. Rate Limiting Per Action: Currently, rate limiting can be handled by web servers (Nginx, Apache). We can have a feature to rate limit users based on logged-in user ID or IP Address in our application. The Rate Limit can be applied separately for separate features / modules. For Example, we can rate limit crowdsource upload per minute, but keep the rate-limit separately for other parts of the application. Captcha Integration: We can integrate Captcha for different actions within our web and mobile applications. We can add captcha verification for registration, running image tests, crowdsource upload, contribution submission etc.","title":"Web Platform Updates"},{"location":"developer-guide/","text":"Introduction Intelligent Supervision Assistant for Construction - Sistema Inteligente de Monitoreo de Obra ISAC-SIMO is a system to validate that the intervention work done for homeowners has been done correctly and safely. It is a Build Change project supported by a grant from IBM. BEFORE YOU START Before starting the project, you need to set these requirements. Python = 3.9 PostgreSQL > 13.x Installation View the Open-Source GitHub repository for ISAC-SIMO Django Backend . First Clone this Project in a suitable directory & Change to project directory. git clone https://github.com/ISAC-SIMO/ISAC-SIMO-Django-Backend.git cd ISAC-SIMO-Django-Backend Make sure you have installed Python 3.9.0 To manage multiple Python installation use pyenv or pyenv-win (for windows). Then, pipenv should be able to automatically install required python. We use Pipenv for managing the dependencies. These are the steps to install & setup pipenv . pip install --upgrade pip pip install pipenv pipenv install --python 3.9 pipenv run install-client It will setup a virtual environment, installs correct python version using pyenv , installs all required packages and libraries. It might take some time on first setup. For CentOS and other cloud servers that only support psycopg2-binary , use this instead: pipenv run install-server Further more packages might need to be installed so that your custom python scripts pre/post processors and offline model work properly. You can install pip packages using: pipenv run pip install <package_name> To enter into virtual environment shell pipenv provides this command: pipenv shell Inside the shell you can run pip , python or any other command directly. You can use pipenv run python manage.py check to verify if the app runs successfully or not. Configuration You will need to set up a .env file with required configurations before starting the application. First, make a copy of .env.example cp .env.example .env Change cp to copy for windows Modify the .env file as required ENV = \u201clocal\u201d or \u201cproduction\u201d (Debug is only enabled in local environment by default) SECRET_KEY = Generate a key with https://djecrety.ir/ DATABASE_URL = You can either modify isac_simo/database_settings.py file or override it by providing database url here. Make sure you have created the database successfully. IBM_API_KEY = Watson API Key to use by default (i.e. if not provided in any project / classifiers) IBM_BUCKET_ENDPOINT = IBM COS Endpoint IBM_BUCKET = COS Bucket Name IBM_BUCKET_TOKEN = COS Key/Token IBM_BUCKET_CRN = COS Bucket CRN IBM_BUCKET_PUBLIC_ENDPOINT = COS Bucket Public Endpoint (Bucket access policies should have \"Public Access\" enabled as an \"Object Reader\") PROJECT_FOLDER = Specify the Relative Path to Project Folder (e.g. /home/username/isac). Do not end the path with closing front or back slash ('/' or '\\') MAINTENANCE = Enable or Disable Maintenance Mode PASSWORD = A Secret Password used for Webhook Requests GOOGLE_MAP_STREET_API = API Key for Google Map Street Views GOOGLE_MAP_API = API Key for Google Map Web CACHE_LOCATION = Relative Path to Store Cache, And also used to Sync Data between Threads (e.g. /var/tmp/django_cache Make sure the Path exists) Migrating & Running Now, if the environment is set up properly, you can migrate the tables to your database. Note: that if you are inside pipenv shell you do not need to specify pipenv run everytime. pipenv run python manage.py migrate And, Create a Super-User to get started. pipenv run python manage.py createsuperuser Now, start the Application with: pipenv run python manage.py runserver And visit http://127.0.0.1:8000/ in any modern browsers to open the application. Note : /static/ and /media/ are the static files and media files location respectively. Write & Running Unit Test Case If the check & migration ran properly. Run Test Case with: pipenv run python manage.py test --debug-mode --debug-sql --parallel --buffer OR pipenv run test OR simply run without any flags: pipenv run python manage.py test Learn how to write Django Test Cases in the Django Official Documentation . Example Model TestCase : from django.test import TestCase from api.models import Image from main.models import User class TestImageFileTest(TestCase): def setUp(self): user = User.objects.create_user(email=\"testuser@gmail.com\", user_type=\"user\", password=\"test@1234\") Image.objects.create(title=\"test title\", description=\"test desc\", user=user, lat=26, lng=84) def test_image_created(self): self.assertEqual(Image.objects.count(), 1) Example URL TestCase : from django.test import TestCase from django.urls import resolve, reverse from api import views class TestImageRelatedUrl(TestCase): def test_images_resloved(self): url = reverse('images') self.assertEqual(resolve(url).func, views.images) Example API Token TestCase : from django.urls import reverse from rest_framework import status from rest_framework.test import APITestCase from main.models import User class TestUserAccountAPI(APITestCase): def setUp(self): User.objects.create_user(email=\"testuser@gmail.com\", user_type=\"user\", password=\"test@1234\") def test_get_token(self): response = self.client.post(reverse('auth'), {\"email\": \"testuser@gmail.com\", \"password\": \"test@1234\"}) self.assertEqual(response.status_code, status.HTTP_200_OK) Database Above is a rough Database Table Relationship Diagram. The Tables and their purpose are described below. Here, Django Model is representing respective tables and all available Columns and Getters can be viewed in Django Model Files. User : Stores the users themselves, their information and user type. It is used to Login users. User Types can be any of \"user\", \"engineer\", \u201cgovernment\", \u201cproject_admin\", \u201cadmin\". Projects : Stores Project Information along with guest & public boolean fields in it. Image : The tested images information like title, latitude, longitude, description, added by etc. ImageFile : Many-To-One Relation with Image Model, which stores the individual image data. The file location, result, score, pipeline, verified status etc. are some fields. OfflineModel : Stores the Offline/Local Models information and file location along with the model types, format, label etc. ObjectType : Stores Object information like name, instruction, image, wishlist & verified status along with link to chosen Project. Classifier : Stores all Classifier/Model information which includes linking of Offline Model, Watson Classifier, Watson Object Detection with specific Object Type. The Order column determines in which order the Classifier will run in the Pipeline (i.e. Pipeline of Same Object Type) FileUpload : Stores any other files (useful for Helper Models & Scripts) that can be uploaded. Crowdsource : Stores Images uploaded in Crowdsource by users with different information. Contribution : Stores Contribution files and information for specific Object Type of a Project Django Apps The Installed Apps used in this project can be found inside \u201cmain\u201d , \u201dprojects\u201d , \u201capi\u201d , \u201cmap\u201d , \u201ccrowdsource\u201d directories. These apps have their own purposes. These directories contain their own templates, models, forms, serializers, urls, views etc. The \u201cmain\u201d app mostly handles User Model, Registration, Login and User Management by Admin. The \u201cprojects\u201d app handles Project Model and its CRUD, Public Projects & Contributions. The \u201cmap\u201d app handles Google Map Street View based image testing using a selected pipeline. The \u201ccrowdsource\u201d app handles Crowdsource Model, API and all of its controllers. The \u201capi\u201d app is the largest among all, which handles all remaining models (Image, Image File, Object Type, Classifier, Offline Model, File Upload and Contribution). It contains all controllers, API serializers and views to handle all requests. It also contains a helpers.py file that handles all of Image Tests, Watson API Requests, Pipeline Recursion, running Detect Model, running Classifier, Offline Model tests, Image Processing, Testing Models (Quick Tests), Re-Training & Creating Watson Model and more. Files and their Uses: urls.py - Handle routes for both web and api views.py - Controller for both web and api forms.py - Manages Django Web Forms models.py - Manages Models (Database Table Object) serializers.py - Manages api serializer for different models along with create, edit functions. middleware.py - Contains middleware handlers (like Maintenance Mode Detection etc.) helpers.py - Many helper functions like watson api call, offline model run, tests etc. migrations/* - Contains all Database Migrations. templates/*.html - Contains Django Web Templates & Master wrapper. Getting started with IBM Watson You know that, with ISAC-SIMO you can create a model pipeline to test provided images. These models/classifiers can be local offline models like h5, python script etc. Or, you can also use IBM Watson Classifier and Object Detection Model, as in a lot of cases this can be convenient and even better. To create your own IBM Watson Classifiers and Object Detection Model, you should first register and create your account. Visit here to learn more about Watson https://www.ibm.com/watson After registering, you can create a new Watson Studio here, https://cloud.ibm.com/catalog/services/watson-studio On Successful Watson Studio creation, you are able to access this page via Resource List. The API Key can be copied and used in either IBM_API_KEY environment variable, Or users can also use this while Creating/Editing Project and Classifier Model Clicking the \u201cLaunch Watson Studio\u201d button you can open the Watson dashboard, where classifiers and object detection models can be created and trained. You can learn more about Watson from their documentation. After you have trained a new Model, you can use it in the ISAC-SIMO application. Just copy the \u201ccollection ID\u201d or \u201cclassifier ID\u201d and use it while creating the Pre-Trained Model. Then put the API Key as shown before. (You can also test Pre-Built Models) ISAC-SIMO also allows you to create a Classifier from the app itself, which might be easier. Now, you should be able to create and use Watson models with ISAC-SIMO easily. How ISAC-SIMO Calls Watson Model There are multiple ways to work with Watson via Python Script. ISAC-SIMO uses simple API requests attaching files and content to the request body. Documentation Here Most of all API calls are handled by api/helpers.py file. It uses requests library to send header, data, files as required for the watson api. As per the Model/Classifier added by ISAC-SIMO users, it can determine whether to call Watson API or run Pre/Post Processor or Python Scripts from the pipeline. How ISAC-SIMO Runs the Model Pipeline As we know, different Object Types can have different pipelines of classifiers that are run when testing user uploaded images. When creating these classifiers/models for any object type, admin or project-admin can provide the order in which the model runs. Programmatically, we use recursion techniques to run pipelines in order. The api/helpers.py file has a function named test_image which runs the test on uploaded images and recursively passes the model response to the next model in the pipeline. If it is a preprocessor model, it sends the image and expects an image to be returned in opencv format. Similarly, postprocessors should return their own standard response. If the model is watson classifier or object detection then it is sent via api and parses the response as required. For More information refer to even more detailed Documentation Here which also shows examples of offline/local models. How to Deploy & Manage ISAC-SIMO in a VPC (Basics) This section only explains the basics, on how to deploy ISAC-SIMO Django Application to a fresh VPC. Create Virtual Private Cloud with IBM Cloud: You can get started with IBM VPC Here From the VPCs tab, you can view and create new VPC for your server. While creating a new VPC, you need to choose to enable SSH, Ping, choose an appropriate server, IP range and enable Public Gateway. Click on the recently clicked VPC and create Subnets for your server. Now, you need to add your SSH Key so that you can shh to any VPC instance you will create. Using the SSH Keys tab you can manage ssh access. Now, you can create a New Virtual Server Instance using the sidebar tab When creating a new instance, choose an appropriate Operating System (Linux is preferable). Select the SSH key you added, choose desired Profile/Server Type and choose the VPC you created earlier. This should set up a new VPC Instance. To ssh and access this instance from the web you need to create a floating IP and assign the VPC to it. From the Floating IPs sidebar tab you can create and manage Floating ips . Now, in the Virtual Server Instances page you should see Floating IP in the table. You can then SSH to the server, using the Floating IP, ssh xx.xx.xx.xx As most of the VPC will be a fresh installation, you might need to install git , python3 , nano and other basic commands and tools before you get started. Just like installing in the local machine on linux, you first need to Clone the project in the desired directory. Make sure there is proper access right to the directory. Then firstly, make sure you have postgres server installed and running. Create a database and put DATABASE_URL in the .env file. Then, create a virtual environment, install the requirements, configure .env file, migrate and create a super user. In most VPC you should also enable port 80 using firewall-cmd or similar commands. These documentations will be handy while setting up a fresh VPC with Django. https://www.digitalocean.com/community/tutorials/how-to-set-up-django-with-postgres-nginx-and-gunicorn-on-ubuntu-16-04 https://www.redhat.com/en/blog/setting-django-application-rhel-8-beta https://www.digitalocean.com/community/tutorials/how-to-secure-nginx-with-let-s-encrypt-on-centos-7 It will help you set up Gunicorn and Nginx with Supervisord to run the django application at port 80. And, the 3rd link will help you set up and install a let's encrypt certificate in your server with auto update. Log Rotate, Error Pages, Managing unnecessary open ports are some other things you might want to do.","title":"Developer Guide"},{"location":"developer-guide/#introduction","text":"Intelligent Supervision Assistant for Construction - Sistema Inteligente de Monitoreo de Obra ISAC-SIMO is a system to validate that the intervention work done for homeowners has been done correctly and safely. It is a Build Change project supported by a grant from IBM.","title":"Introduction"},{"location":"developer-guide/#before-you-start","text":"Before starting the project, you need to set these requirements. Python = 3.9 PostgreSQL > 13.x","title":"BEFORE YOU START"},{"location":"developer-guide/#installation","text":"View the Open-Source GitHub repository for ISAC-SIMO Django Backend . First Clone this Project in a suitable directory & Change to project directory. git clone https://github.com/ISAC-SIMO/ISAC-SIMO-Django-Backend.git cd ISAC-SIMO-Django-Backend Make sure you have installed Python 3.9.0 To manage multiple Python installation use pyenv or pyenv-win (for windows). Then, pipenv should be able to automatically install required python. We use Pipenv for managing the dependencies. These are the steps to install & setup pipenv . pip install --upgrade pip pip install pipenv pipenv install --python 3.9 pipenv run install-client It will setup a virtual environment, installs correct python version using pyenv , installs all required packages and libraries. It might take some time on first setup. For CentOS and other cloud servers that only support psycopg2-binary , use this instead: pipenv run install-server Further more packages might need to be installed so that your custom python scripts pre/post processors and offline model work properly. You can install pip packages using: pipenv run pip install <package_name> To enter into virtual environment shell pipenv provides this command: pipenv shell Inside the shell you can run pip , python or any other command directly. You can use pipenv run python manage.py check to verify if the app runs successfully or not.","title":"Installation"},{"location":"developer-guide/#configuration","text":"You will need to set up a .env file with required configurations before starting the application. First, make a copy of .env.example cp .env.example .env Change cp to copy for windows Modify the .env file as required ENV = \u201clocal\u201d or \u201cproduction\u201d (Debug is only enabled in local environment by default) SECRET_KEY = Generate a key with https://djecrety.ir/ DATABASE_URL = You can either modify isac_simo/database_settings.py file or override it by providing database url here. Make sure you have created the database successfully. IBM_API_KEY = Watson API Key to use by default (i.e. if not provided in any project / classifiers) IBM_BUCKET_ENDPOINT = IBM COS Endpoint IBM_BUCKET = COS Bucket Name IBM_BUCKET_TOKEN = COS Key/Token IBM_BUCKET_CRN = COS Bucket CRN IBM_BUCKET_PUBLIC_ENDPOINT = COS Bucket Public Endpoint (Bucket access policies should have \"Public Access\" enabled as an \"Object Reader\") PROJECT_FOLDER = Specify the Relative Path to Project Folder (e.g. /home/username/isac). Do not end the path with closing front or back slash ('/' or '\\') MAINTENANCE = Enable or Disable Maintenance Mode PASSWORD = A Secret Password used for Webhook Requests GOOGLE_MAP_STREET_API = API Key for Google Map Street Views GOOGLE_MAP_API = API Key for Google Map Web CACHE_LOCATION = Relative Path to Store Cache, And also used to Sync Data between Threads (e.g. /var/tmp/django_cache Make sure the Path exists)","title":"Configuration"},{"location":"developer-guide/#migrating-running","text":"Now, if the environment is set up properly, you can migrate the tables to your database. Note: that if you are inside pipenv shell you do not need to specify pipenv run everytime. pipenv run python manage.py migrate And, Create a Super-User to get started. pipenv run python manage.py createsuperuser Now, start the Application with: pipenv run python manage.py runserver And visit http://127.0.0.1:8000/ in any modern browsers to open the application. Note : /static/ and /media/ are the static files and media files location respectively.","title":"Migrating &amp; Running"},{"location":"developer-guide/#write-running-unit-test-case","text":"If the check & migration ran properly. Run Test Case with: pipenv run python manage.py test --debug-mode --debug-sql --parallel --buffer OR pipenv run test OR simply run without any flags: pipenv run python manage.py test Learn how to write Django Test Cases in the Django Official Documentation . Example Model TestCase : from django.test import TestCase from api.models import Image from main.models import User class TestImageFileTest(TestCase): def setUp(self): user = User.objects.create_user(email=\"testuser@gmail.com\", user_type=\"user\", password=\"test@1234\") Image.objects.create(title=\"test title\", description=\"test desc\", user=user, lat=26, lng=84) def test_image_created(self): self.assertEqual(Image.objects.count(), 1) Example URL TestCase : from django.test import TestCase from django.urls import resolve, reverse from api import views class TestImageRelatedUrl(TestCase): def test_images_resloved(self): url = reverse('images') self.assertEqual(resolve(url).func, views.images) Example API Token TestCase : from django.urls import reverse from rest_framework import status from rest_framework.test import APITestCase from main.models import User class TestUserAccountAPI(APITestCase): def setUp(self): User.objects.create_user(email=\"testuser@gmail.com\", user_type=\"user\", password=\"test@1234\") def test_get_token(self): response = self.client.post(reverse('auth'), {\"email\": \"testuser@gmail.com\", \"password\": \"test@1234\"}) self.assertEqual(response.status_code, status.HTTP_200_OK)","title":"Write &amp; Running Unit Test Case"},{"location":"developer-guide/#database","text":"Above is a rough Database Table Relationship Diagram. The Tables and their purpose are described below. Here, Django Model is representing respective tables and all available Columns and Getters can be viewed in Django Model Files. User : Stores the users themselves, their information and user type. It is used to Login users. User Types can be any of \"user\", \"engineer\", \u201cgovernment\", \u201cproject_admin\", \u201cadmin\". Projects : Stores Project Information along with guest & public boolean fields in it. Image : The tested images information like title, latitude, longitude, description, added by etc. ImageFile : Many-To-One Relation with Image Model, which stores the individual image data. The file location, result, score, pipeline, verified status etc. are some fields. OfflineModel : Stores the Offline/Local Models information and file location along with the model types, format, label etc. ObjectType : Stores Object information like name, instruction, image, wishlist & verified status along with link to chosen Project. Classifier : Stores all Classifier/Model information which includes linking of Offline Model, Watson Classifier, Watson Object Detection with specific Object Type. The Order column determines in which order the Classifier will run in the Pipeline (i.e. Pipeline of Same Object Type) FileUpload : Stores any other files (useful for Helper Models & Scripts) that can be uploaded. Crowdsource : Stores Images uploaded in Crowdsource by users with different information. Contribution : Stores Contribution files and information for specific Object Type of a Project","title":"Database"},{"location":"developer-guide/#django-apps","text":"The Installed Apps used in this project can be found inside \u201cmain\u201d , \u201dprojects\u201d , \u201capi\u201d , \u201cmap\u201d , \u201ccrowdsource\u201d directories. These apps have their own purposes. These directories contain their own templates, models, forms, serializers, urls, views etc. The \u201cmain\u201d app mostly handles User Model, Registration, Login and User Management by Admin. The \u201cprojects\u201d app handles Project Model and its CRUD, Public Projects & Contributions. The \u201cmap\u201d app handles Google Map Street View based image testing using a selected pipeline. The \u201ccrowdsource\u201d app handles Crowdsource Model, API and all of its controllers. The \u201capi\u201d app is the largest among all, which handles all remaining models (Image, Image File, Object Type, Classifier, Offline Model, File Upload and Contribution). It contains all controllers, API serializers and views to handle all requests. It also contains a helpers.py file that handles all of Image Tests, Watson API Requests, Pipeline Recursion, running Detect Model, running Classifier, Offline Model tests, Image Processing, Testing Models (Quick Tests), Re-Training & Creating Watson Model and more.","title":"Django Apps"},{"location":"developer-guide/#files-and-their-uses","text":"urls.py - Handle routes for both web and api views.py - Controller for both web and api forms.py - Manages Django Web Forms models.py - Manages Models (Database Table Object) serializers.py - Manages api serializer for different models along with create, edit functions. middleware.py - Contains middleware handlers (like Maintenance Mode Detection etc.) helpers.py - Many helper functions like watson api call, offline model run, tests etc. migrations/* - Contains all Database Migrations. templates/*.html - Contains Django Web Templates & Master wrapper.","title":"Files and their Uses:"},{"location":"developer-guide/#getting-started-with-ibm-watson","text":"You know that, with ISAC-SIMO you can create a model pipeline to test provided images. These models/classifiers can be local offline models like h5, python script etc. Or, you can also use IBM Watson Classifier and Object Detection Model, as in a lot of cases this can be convenient and even better. To create your own IBM Watson Classifiers and Object Detection Model, you should first register and create your account. Visit here to learn more about Watson https://www.ibm.com/watson After registering, you can create a new Watson Studio here, https://cloud.ibm.com/catalog/services/watson-studio On Successful Watson Studio creation, you are able to access this page via Resource List. The API Key can be copied and used in either IBM_API_KEY environment variable, Or users can also use this while Creating/Editing Project and Classifier Model Clicking the \u201cLaunch Watson Studio\u201d button you can open the Watson dashboard, where classifiers and object detection models can be created and trained. You can learn more about Watson from their documentation. After you have trained a new Model, you can use it in the ISAC-SIMO application. Just copy the \u201ccollection ID\u201d or \u201cclassifier ID\u201d and use it while creating the Pre-Trained Model. Then put the API Key as shown before. (You can also test Pre-Built Models) ISAC-SIMO also allows you to create a Classifier from the app itself, which might be easier. Now, you should be able to create and use Watson models with ISAC-SIMO easily.","title":"Getting started with IBM Watson"},{"location":"developer-guide/#how-isac-simo-calls-watson-model","text":"There are multiple ways to work with Watson via Python Script. ISAC-SIMO uses simple API requests attaching files and content to the request body. Documentation Here Most of all API calls are handled by api/helpers.py file. It uses requests library to send header, data, files as required for the watson api. As per the Model/Classifier added by ISAC-SIMO users, it can determine whether to call Watson API or run Pre/Post Processor or Python Scripts from the pipeline.","title":"How ISAC-SIMO Calls Watson Model"},{"location":"developer-guide/#how-isac-simo-runs-the-model-pipeline","text":"As we know, different Object Types can have different pipelines of classifiers that are run when testing user uploaded images. When creating these classifiers/models for any object type, admin or project-admin can provide the order in which the model runs. Programmatically, we use recursion techniques to run pipelines in order. The api/helpers.py file has a function named test_image which runs the test on uploaded images and recursively passes the model response to the next model in the pipeline. If it is a preprocessor model, it sends the image and expects an image to be returned in opencv format. Similarly, postprocessors should return their own standard response. If the model is watson classifier or object detection then it is sent via api and parses the response as required. For More information refer to even more detailed Documentation Here which also shows examples of offline/local models.","title":"How ISAC-SIMO Runs the Model Pipeline"},{"location":"developer-guide/#how-to-deploy-manage-isac-simo-in-a-vpc-basics","text":"This section only explains the basics, on how to deploy ISAC-SIMO Django Application to a fresh VPC.","title":"How to Deploy &amp; Manage ISAC-SIMO in a VPC (Basics)"},{"location":"developer-guide/#create-virtual-private-cloud-with-ibm-cloud","text":"You can get started with IBM VPC Here From the VPCs tab, you can view and create new VPC for your server. While creating a new VPC, you need to choose to enable SSH, Ping, choose an appropriate server, IP range and enable Public Gateway. Click on the recently clicked VPC and create Subnets for your server. Now, you need to add your SSH Key so that you can shh to any VPC instance you will create. Using the SSH Keys tab you can manage ssh access. Now, you can create a New Virtual Server Instance using the sidebar tab When creating a new instance, choose an appropriate Operating System (Linux is preferable). Select the SSH key you added, choose desired Profile/Server Type and choose the VPC you created earlier. This should set up a new VPC Instance. To ssh and access this instance from the web you need to create a floating IP and assign the VPC to it. From the Floating IPs sidebar tab you can create and manage Floating ips . Now, in the Virtual Server Instances page you should see Floating IP in the table. You can then SSH to the server, using the Floating IP, ssh xx.xx.xx.xx As most of the VPC will be a fresh installation, you might need to install git , python3 , nano and other basic commands and tools before you get started. Just like installing in the local machine on linux, you first need to Clone the project in the desired directory. Make sure there is proper access right to the directory. Then firstly, make sure you have postgres server installed and running. Create a database and put DATABASE_URL in the .env file. Then, create a virtual environment, install the requirements, configure .env file, migrate and create a super user. In most VPC you should also enable port 80 using firewall-cmd or similar commands. These documentations will be handy while setting up a fresh VPC with Django. https://www.digitalocean.com/community/tutorials/how-to-set-up-django-with-postgres-nginx-and-gunicorn-on-ubuntu-16-04 https://www.redhat.com/en/blog/setting-django-application-rhel-8-beta https://www.digitalocean.com/community/tutorials/how-to-secure-nginx-with-let-s-encrypt-on-centos-7 It will help you set up Gunicorn and Nginx with Supervisord to run the django application at port 80. And, the 3rd link will help you set up and install a let's encrypt certificate in your server with auto update. Log Rotate, Error Pages, Managing unnecessary open ports are some other things you might want to do.","title":"Create Virtual Private Cloud with IBM Cloud:"},{"location":"getting-started/","text":"Getting Started Introduction This section will guide you through the steps to create a simple Project with Rebar Shape Quality Checker. It will cover creating a project, adding and linking a classifier, adding and linking a offline model, ordering the pipeline and testing an image via mobile application. STEP 1 - Register as project Admin \ud83d\udec8 More info on Login & Register In the Registration page fill up your Details, Email and Password. Then, choose Type as \u201cProject Admin\u201d so that you can create and manage projects and models. A Project Admin will have access to other user details, so the user must be first verified by an Admin. Wait for Admin to verify your account, so that you can login. After the account is verified you can login and access the Dashboard. You can also register as Type \u201cUser\u201d if you want. But, you can only test images after you are linked to a Project by Admin or Project Admin. Note : Password needs to be strong with Upper Case, Numbers and Special Characters. STEP 2 - Create a New Project \ud83d\udec8 More info on Projects As you registered as a Project Admin, you can create and manage multiple projects. From the Sidebar, you can access View Projects and Create Project Page. Lets create a New Project as shown in this Image. Fill in the Project Name, Description and Image. Now, you can either link a Watson Object Detect Model with IBM API Access Key. Or, add and link Offline Model of type Object Detect. Then, when testing an image using the API you can choose to force an object type by providing object_type_id or choose to use this Project Object Detection Model providing the project_id. Now, if you visit the View Projects page you can see this newly created Project with a hint that you need to add Object Types and Classifiers which we will do next. (You can also test linked Object Detect Model) STEP 3 - Add Object Type \ud83d\udec8 More info on Object Types You have created a Project, but you need to add Object Types that this project will test. For Example, in our \u201cMy Wall Test Project\u201d, users will be able to test Wall Facade, Wall Bond Pattern etc. These are the Object Types that can be tested. You can view and manage the Object Types as shown in Image below. Here, we add Object Types, choose the Project and provide suitable Instruction and Image on how to perform the test. You can choose multiple countries in which this check should be available. Depending on users GPS location, it will show or hide certain object type from check list. Now, the Classifiers/Models Pipeline for these specific Object Types can be added so that users can test. STEP 4 - Add Classifiers & Pipeline \ud83d\udec8 More info on Models / Classifiers As we have added the Object Types, when we login from the Mobile Application or use API we can see the option to choose these object types for testing images. Currently, no Classifier/Model has been linked to this Object Types pipeline so the tested result will be empty. So for that, let\u2019s start adding classifiers/models. We can add multiple models, watson models, offline models, pre/post processors and order the pipeline appropriately. For this example, we will add models for \u201cWall Bond Pattern Test\u201d. We have created two python 3 scripts, one to Pre-Process the Image of Brick Wall and the other to Post-Process and Classify the image and return GO/NOGO Result. As these are Offline Model (Not Watson), we first need to add these in Create Offline Model Page. \ud83d\udec8 More info on Offline Models After we have added the Pre/Post Processor appropriately we can quickly test an image and view python dependencies. Make sure that when creating Offline Models, study the Readme guide and view examples. Here, we see that we created one Pre-Processor and one Post-Processor. Now, finally we can create a Model/Classifier and link these offline models. You could ofcourse use Watson Model or Train yourself with Images. In the create form choose the Source of Model, fill the form as required and set the order in which to run. Pre-Processor Post-Processor If you want to easily change the order of the pipeline (Very useful for multiple pipelines), you can do so in the Object Type List View by clicking the \u201cQuick Order Classifier Icon\u201d . There you can drag and arrange the Model and Classifiers easily. STEP 5 - Test with Mobile Application Now, login from the Mobile Application, inside Quality Check you should be able to see \u201cWall Bond Pattern\u201d in the list. Click the item, upload or capture an image of a brick wall and send to test. If all the Classifiers ran successfully, then it should return the GO/NOGO response. \ud83d\udec8 More info on Mobile Applications Quality Check Result After the test, you can also view the result and pipeline specific output in the View Images/Edit/Info page from the web application. You should now be able to add other users, link to your project, create multiple object types, classifiers, models and perform checks easily.","title":"Getting Started"},{"location":"getting-started/#getting-started-introduction","text":"This section will guide you through the steps to create a simple Project with Rebar Shape Quality Checker. It will cover creating a project, adding and linking a classifier, adding and linking a offline model, ordering the pipeline and testing an image via mobile application.","title":"Getting Started Introduction"},{"location":"getting-started/#step-1-register-as-project-admin","text":"\ud83d\udec8 More info on Login & Register In the Registration page fill up your Details, Email and Password. Then, choose Type as \u201cProject Admin\u201d so that you can create and manage projects and models. A Project Admin will have access to other user details, so the user must be first verified by an Admin. Wait for Admin to verify your account, so that you can login. After the account is verified you can login and access the Dashboard. You can also register as Type \u201cUser\u201d if you want. But, you can only test images after you are linked to a Project by Admin or Project Admin. Note : Password needs to be strong with Upper Case, Numbers and Special Characters.","title":"STEP 1 - Register as project Admin"},{"location":"getting-started/#step-2-create-a-new-project","text":"\ud83d\udec8 More info on Projects As you registered as a Project Admin, you can create and manage multiple projects. From the Sidebar, you can access View Projects and Create Project Page. Lets create a New Project as shown in this Image. Fill in the Project Name, Description and Image. Now, you can either link a Watson Object Detect Model with IBM API Access Key. Or, add and link Offline Model of type Object Detect. Then, when testing an image using the API you can choose to force an object type by providing object_type_id or choose to use this Project Object Detection Model providing the project_id. Now, if you visit the View Projects page you can see this newly created Project with a hint that you need to add Object Types and Classifiers which we will do next. (You can also test linked Object Detect Model)","title":"STEP 2 - Create a New Project"},{"location":"getting-started/#step-3-add-object-type","text":"\ud83d\udec8 More info on Object Types You have created a Project, but you need to add Object Types that this project will test. For Example, in our \u201cMy Wall Test Project\u201d, users will be able to test Wall Facade, Wall Bond Pattern etc. These are the Object Types that can be tested. You can view and manage the Object Types as shown in Image below. Here, we add Object Types, choose the Project and provide suitable Instruction and Image on how to perform the test. You can choose multiple countries in which this check should be available. Depending on users GPS location, it will show or hide certain object type from check list. Now, the Classifiers/Models Pipeline for these specific Object Types can be added so that users can test.","title":"STEP 3 - Add Object Type"},{"location":"getting-started/#step-4-add-classifiers-pipeline","text":"\ud83d\udec8 More info on Models / Classifiers As we have added the Object Types, when we login from the Mobile Application or use API we can see the option to choose these object types for testing images. Currently, no Classifier/Model has been linked to this Object Types pipeline so the tested result will be empty. So for that, let\u2019s start adding classifiers/models. We can add multiple models, watson models, offline models, pre/post processors and order the pipeline appropriately. For this example, we will add models for \u201cWall Bond Pattern Test\u201d. We have created two python 3 scripts, one to Pre-Process the Image of Brick Wall and the other to Post-Process and Classify the image and return GO/NOGO Result. As these are Offline Model (Not Watson), we first need to add these in Create Offline Model Page. \ud83d\udec8 More info on Offline Models After we have added the Pre/Post Processor appropriately we can quickly test an image and view python dependencies. Make sure that when creating Offline Models, study the Readme guide and view examples. Here, we see that we created one Pre-Processor and one Post-Processor. Now, finally we can create a Model/Classifier and link these offline models. You could ofcourse use Watson Model or Train yourself with Images. In the create form choose the Source of Model, fill the form as required and set the order in which to run. Pre-Processor Post-Processor If you want to easily change the order of the pipeline (Very useful for multiple pipelines), you can do so in the Object Type List View by clicking the \u201cQuick Order Classifier Icon\u201d . There you can drag and arrange the Model and Classifiers easily.","title":"STEP 4 - Add Classifiers &amp; Pipeline"},{"location":"getting-started/#step-5-test-with-mobile-application","text":"Now, login from the Mobile Application, inside Quality Check you should be able to see \u201cWall Bond Pattern\u201d in the list. Click the item, upload or capture an image of a brick wall and send to test. If all the Classifiers ran successfully, then it should return the GO/NOGO response. \ud83d\udec8 More info on Mobile Applications Quality Check Result After the test, you can also view the result and pipeline specific output in the View Images/Edit/Info page from the web application. You should now be able to add other users, link to your project, create multiple object types, classifiers, models and perform checks easily.","title":"STEP 5 - Test with Mobile Application"},{"location":"integration/","text":"Integration with other Application ISAC-SIMO API is a fully featured rest service which allows other applications to use the public api to add, edit and manage data and records externally. Learn more about the API provided by ISAC-SIMO in Mobile Api Guide . This examples below shows a demonstration on how we can easily integrate ISAC-SIMO into different applications specially the image testing: KoboToolbox \u25b6\ufe0f Watch Video KoboToolbox (kf.kobotoolbox.org) has a feature called Rest Services which allows us to integrate ISAC-SIMO with which it calls our API on each new submission added to the kobo form. First, we need to make sure that the form contains Photo input with the data column name set to \u201cisac_image_xxxx\u201d where \u201cxxxx\u201d can be a unique identifier. The Form can contain multiple ISAC-SIMO test-able image upload fields with \u201cxxxx\u201d being unique for each field. Then, it is important to have a Select One field with the data name set to \"isac_object_xxxx\" (xxxx being the same as that of Photo field). There can be choices added to this field with Text Value and valid XML value same as Check (Object Type) ID from ISAC-SIMO Dashboard. Users can choose the Check from this dropdown and upload the photo. If you want to receive back the ISAC-SIMO test result and store it in a field in the kobo submission record, a hidden text input field with data column name \u201cisac_result_xxxx\u201d can be created. The \"xxxx\" value must be the same as the image upload field. Example we can have: \"isac_image_1\" as photo upload field, \"isac_object_1\" as select one dropdown field and \"isac_result_1\" as the input field to store the result. When uploading, make sure that the isac_result_xxx field is not empty (e.g. add N/A as default value as seen in above screenshot) . Then, in the Kobo Rest Service we can then use the following endpoint. https://www.isac-simo.net/api/kobo/?token=[kobo_token]&domain=[kobo_server_domain] The token should be the Kobo Toolbox auth token that can be found in Account Settings of Kobo user dashboard. Domain used by ISAC-SIMO for sending back the result is https://kc.kobotoolbox.org by default. If you want to change the domain and use your custom server then provide a domain query parameter. The domain in the query parameter must NOT have ending slash. The test result can be viewed in ISAC-SIMO Dashboard with description set to \u201cKoboToolbox / ID\u201d. You can search by _id value here. If \u201cisac_result_xxxx\u201d is valid then the result field will be set in kobo toolbox data also. It might take a few minutes for it to update / sync. Fulcrum \u25b6\ufe0f Watch Video Fulcrum is a popular Data Collection application that has a wide range of features. Integrating ISAC-SIMO into any Fulcrum project is pretty straight-forward. Most of the logic and standard are similar to that of the KoboToolbox method mentioned above. First, we need to make sure that the form contains a Photos field with the data name set to \u201cisac_image_xxxx\u201d where \u201cxxxx\u201d can be a unique identifier. The photo field for faster performance should have a single maximum photo allowed. The Form can contain multiple ISAC-SIMO test-able photo upload fields with \u201cxxxx\u201d being unique for each field. Then, it is important to have a Single Choice field with the data name set to \"isac_object_xxxx\" (xxxx being the same as that of Photo field). There can be choices added to this field with Text Value and valid Check (Object Type) ID from ISAC-SIMO Dashboard. Users can choose the Check from this dropdown and upload the photo. If you want to receive back the ISAC-SIMO test result and store it in a field in fulcrum record, a read-only or hidden text input field with data name \u201cisac_result_xxxx\u201d can be created. The \"xxxx\" value must be the same as the image upload field. Example we can have: \"isac_image_1\" as photo upload field, \"isac_object_1\" as single choice field and \"isac_result_1\" as the input field to store the result. Fulcrum has a feature called \"Webhook\", with which on any event like; create, edit etc. fulcrum can call ISAC-SIMO API with form data. ISAC-SIMO performs tests on photos if valid data names are provided and updates the record if valid data name for result field is provided. https://www.isac-simo.net/api/fulcrum/?token=[token] The token should be the Fulcrums API token that can be found in the Settings / API section of the dashboard. If you need to authenticate an ISAC-SIMO call (for example to call private check by ID) you can add another parameter \"isac_token\" with JWT token generated from ISAC-SIMO users profile page. The test result can be viewed in ISAC-SIMO Dashboard with description set to Fulcrum / ID\u201d. You can search by id value here. Any recurring webhooks with the same ID are ignored by rate-limiting. If \u201cisac_result_xxxx\u201d is valid then the result field will be set in fulcrum data also. It might take a few minutes for it to update / sync. Any other services can easily integrate ISAC-SIMO using our Restful API service. Learn More .","title":"External Integration"},{"location":"integration/#integration-with-other-application","text":"ISAC-SIMO API is a fully featured rest service which allows other applications to use the public api to add, edit and manage data and records externally. Learn more about the API provided by ISAC-SIMO in Mobile Api Guide . This examples below shows a demonstration on how we can easily integrate ISAC-SIMO into different applications specially the image testing:","title":"Integration with other Application"},{"location":"integration/#kobotoolbox","text":"\u25b6\ufe0f Watch Video KoboToolbox (kf.kobotoolbox.org) has a feature called Rest Services which allows us to integrate ISAC-SIMO with which it calls our API on each new submission added to the kobo form. First, we need to make sure that the form contains Photo input with the data column name set to \u201cisac_image_xxxx\u201d where \u201cxxxx\u201d can be a unique identifier. The Form can contain multiple ISAC-SIMO test-able image upload fields with \u201cxxxx\u201d being unique for each field. Then, it is important to have a Select One field with the data name set to \"isac_object_xxxx\" (xxxx being the same as that of Photo field). There can be choices added to this field with Text Value and valid XML value same as Check (Object Type) ID from ISAC-SIMO Dashboard. Users can choose the Check from this dropdown and upload the photo. If you want to receive back the ISAC-SIMO test result and store it in a field in the kobo submission record, a hidden text input field with data column name \u201cisac_result_xxxx\u201d can be created. The \"xxxx\" value must be the same as the image upload field. Example we can have: \"isac_image_1\" as photo upload field, \"isac_object_1\" as select one dropdown field and \"isac_result_1\" as the input field to store the result. When uploading, make sure that the isac_result_xxx field is not empty (e.g. add N/A as default value as seen in above screenshot) . Then, in the Kobo Rest Service we can then use the following endpoint. https://www.isac-simo.net/api/kobo/?token=[kobo_token]&domain=[kobo_server_domain] The token should be the Kobo Toolbox auth token that can be found in Account Settings of Kobo user dashboard. Domain used by ISAC-SIMO for sending back the result is https://kc.kobotoolbox.org by default. If you want to change the domain and use your custom server then provide a domain query parameter. The domain in the query parameter must NOT have ending slash. The test result can be viewed in ISAC-SIMO Dashboard with description set to \u201cKoboToolbox / ID\u201d. You can search by _id value here. If \u201cisac_result_xxxx\u201d is valid then the result field will be set in kobo toolbox data also. It might take a few minutes for it to update / sync.","title":"KoboToolbox"},{"location":"integration/#fulcrum","text":"\u25b6\ufe0f Watch Video Fulcrum is a popular Data Collection application that has a wide range of features. Integrating ISAC-SIMO into any Fulcrum project is pretty straight-forward. Most of the logic and standard are similar to that of the KoboToolbox method mentioned above. First, we need to make sure that the form contains a Photos field with the data name set to \u201cisac_image_xxxx\u201d where \u201cxxxx\u201d can be a unique identifier. The photo field for faster performance should have a single maximum photo allowed. The Form can contain multiple ISAC-SIMO test-able photo upload fields with \u201cxxxx\u201d being unique for each field. Then, it is important to have a Single Choice field with the data name set to \"isac_object_xxxx\" (xxxx being the same as that of Photo field). There can be choices added to this field with Text Value and valid Check (Object Type) ID from ISAC-SIMO Dashboard. Users can choose the Check from this dropdown and upload the photo. If you want to receive back the ISAC-SIMO test result and store it in a field in fulcrum record, a read-only or hidden text input field with data name \u201cisac_result_xxxx\u201d can be created. The \"xxxx\" value must be the same as the image upload field. Example we can have: \"isac_image_1\" as photo upload field, \"isac_object_1\" as single choice field and \"isac_result_1\" as the input field to store the result. Fulcrum has a feature called \"Webhook\", with which on any event like; create, edit etc. fulcrum can call ISAC-SIMO API with form data. ISAC-SIMO performs tests on photos if valid data names are provided and updates the record if valid data name for result field is provided. https://www.isac-simo.net/api/fulcrum/?token=[token] The token should be the Fulcrums API token that can be found in the Settings / API section of the dashboard. If you need to authenticate an ISAC-SIMO call (for example to call private check by ID) you can add another parameter \"isac_token\" with JWT token generated from ISAC-SIMO users profile page. The test result can be viewed in ISAC-SIMO Dashboard with description set to Fulcrum / ID\u201d. You can search by id value here. Any recurring webhooks with the same ID are ignored by rate-limiting. If \u201cisac_result_xxxx\u201d is valid then the result field will be set in fulcrum data also. It might take a few minutes for it to update / sync. Any other services can easily integrate ISAC-SIMO using our Restful API service. Learn More .","title":"Fulcrum"},{"location":"lite-dashboard/","text":"ISAC-SIMO-LITE INTRODUCTION Intelligent Supervision Assistant for Construction - Sistema Inteligente de Monitoreo de Obra ISAC-SIMO is a system to validate that the intervention work done for homeowners has been done correctly and safely. It is a Build Change project supported by a grant from IBM. ISAC-SIMO-Lite is a Developer Test Tool that provides a convenient developer test environment to work with pipeline models. Using this Web Application, users can add different models, watson classifiers, processors in a single page dashboard and quickly test images. It shows all the results of each model of the pipeline after the test is successful and any possible errors. \u25b6\ufe0f Watch Video BEFORE YOU START Before starting the project, you need to set these requirements. Python > 3.8.x INSTALLATION View the Open-Source GitHub repository for ISAC-SIMO Dashboard Lite . First Clone this Project in a suitable directory & Change to project directory. git clone https://github.com/ISAC-SIMO/ISAC-SIMO-Dashboard-Lite.git cd ISAC-SIMO-Dashboard-Lite Then, you will need to set up a virtual environment. The easiest way to setup is using pipenv. First, install pipenv using the following command. pip3 install --upgrade setuptools pip3 install pipenv To activate the virtual environment shell you need to run these commands, followed by installing required packages. (Note that every time you run the application server, you need to be inside virtual env shell) pipenv shell pipenv install --skip-lock Now, start the Application with: python manage.py runserver --noreload And visit http://127.0.0.1:8000/ in any modern browsers to open the application. /static/ folder should have proper read and write access to current users or groups. During setup if package installation caused any error, please install them outside of pipenv and try running the server again. And, also note that most of the app controller are kept in /main/views.py GETTING STARTED The application when loaded should look like this. ADDING MODEL PIPELINE You can add new models to the pipeline by clicking on the \u201c+ Add Model Pipeline\u201d button. After clicking on the button, it will add a new card where you can choose the type of model, provide it a name, and fill other fields as required. Provide an appropriate name to the model and choose the type of model as shown below. When you choose the model type, it will add new fields as required by that specific model. For example, a pre-processor needs a python script file, Watson classifier needs IBM API Keys, Collection ID etc. Fill the form as required. You can remove any model from the pipeline by clicking on the \u201c Delete \u201d button. The \u201c Freeze \u201d button is useful to disable the card to prevent accidental clicks and changes. More models can be added to the pipeline by clicking \u201c + Add Model Pipeline \u201d as before and filling the forms. PROVIDING THE TEST IMAGE After you have added all the pipeline models properly, you can provide a test image which is used by the models and classifiers. You can choose the file using the field called \u201c Provide the Test Image \u201d. Choose an appropriate image file for testing. TESTING THE IMAGE Now, the \u201c \u279c Test Image \u201d button can be clicked to test the image using the provided pipeline of models. Any validation errors will be displayed properly with error messages and which model caused it. Note : When you have already chosen a file or image in the web application and later modify it (locally) and try to run the test, it might not pass in chromium browsers (due to permissions issue). The application in that case will show \u201cFile has been modified\u201d error and you need to choose the file again using the picker. If all validation passes, it will start the testing process and on success show the results for each model. Click on the \u201c View Test Result \u25bd \u201d button to toggle its result. It might contain an error message in red font color if the model did not run successfully. Else, it will show the result properly as required. The pre-processor will show the processed image. The post-processor and classifier will show its JSON response. Similarly, Watson classifier and object detection will show its own response along with best result and score. This image shows the test result of the Pre-Processor This image shows the test result of the Watson Classifier Results of the whole pipeline can be seen on the bottom of the page under the title of \u201c Pipeline Result \u201d. You can always modify the model, change the file or image and run the test again without reloading the page. Any unhandled errors might be found in Django Logs. TESTING WITH MOBILE APP Images can be tested either via web application or can also be tested with ISAC-SIMO Lite Mobile App. With the app, you need to provide the Local IP address of your running Django Project. To make the project accessible from local devices, you need to run the server using following command instead: python manage.py runserver 0.0.0.0:8000 --noreload Find your IP address with ipconfig or ifconfig or similar commands and enter that IP address in the mobile app. Then, continue with the Mobile Application by uploading or capturing images of different object types as required. The images will be sent to the running server in that IP address and will pass through all the pipeline models. Make sure that, when you make changes in your model files, web app UI etc. you first need to test the image from the web app, so that the changes get stored in the application for the mobile app to use. Unless you reload the page, you can access the latest mobile api response (i.e. pipeline results) from the web app itself. After you have tested the image from the mobile app, you can click on the \" View Mobile Test Result \" button. This will load the pipeline result of the latest test image and overrides the test results content. If you need, the pipeline configuration is stored in the /static/temp/config.json file. The mobile test result (i.e. pipeline result) is stored in /static/temp/external_response.json file. All the processors, classifiers, test images are stored in the /static/temp folder. LINKS ISAC-SIMO Documentation (Latest) ISAC-SIMO Backend Developer Guide ISAC-SIMO Lite Dashboard GitHub Repository ISAC-SIMO Main GitHub Repository Download PDF","title":"Lite Dashboard"},{"location":"lite-dashboard/#isac-simo-lite","text":"","title":"ISAC-SIMO-LITE"},{"location":"lite-dashboard/#introduction","text":"Intelligent Supervision Assistant for Construction - Sistema Inteligente de Monitoreo de Obra ISAC-SIMO is a system to validate that the intervention work done for homeowners has been done correctly and safely. It is a Build Change project supported by a grant from IBM. ISAC-SIMO-Lite is a Developer Test Tool that provides a convenient developer test environment to work with pipeline models. Using this Web Application, users can add different models, watson classifiers, processors in a single page dashboard and quickly test images. It shows all the results of each model of the pipeline after the test is successful and any possible errors. \u25b6\ufe0f Watch Video","title":"INTRODUCTION"},{"location":"lite-dashboard/#before-you-start","text":"Before starting the project, you need to set these requirements. Python > 3.8.x","title":"BEFORE YOU START"},{"location":"lite-dashboard/#installation","text":"View the Open-Source GitHub repository for ISAC-SIMO Dashboard Lite . First Clone this Project in a suitable directory & Change to project directory. git clone https://github.com/ISAC-SIMO/ISAC-SIMO-Dashboard-Lite.git cd ISAC-SIMO-Dashboard-Lite Then, you will need to set up a virtual environment. The easiest way to setup is using pipenv. First, install pipenv using the following command. pip3 install --upgrade setuptools pip3 install pipenv To activate the virtual environment shell you need to run these commands, followed by installing required packages. (Note that every time you run the application server, you need to be inside virtual env shell) pipenv shell pipenv install --skip-lock Now, start the Application with: python manage.py runserver --noreload And visit http://127.0.0.1:8000/ in any modern browsers to open the application. /static/ folder should have proper read and write access to current users or groups. During setup if package installation caused any error, please install them outside of pipenv and try running the server again. And, also note that most of the app controller are kept in /main/views.py","title":"INSTALLATION"},{"location":"lite-dashboard/#getting-started","text":"The application when loaded should look like this.","title":"GETTING STARTED"},{"location":"lite-dashboard/#adding-model-pipeline","text":"You can add new models to the pipeline by clicking on the \u201c+ Add Model Pipeline\u201d button. After clicking on the button, it will add a new card where you can choose the type of model, provide it a name, and fill other fields as required. Provide an appropriate name to the model and choose the type of model as shown below. When you choose the model type, it will add new fields as required by that specific model. For example, a pre-processor needs a python script file, Watson classifier needs IBM API Keys, Collection ID etc. Fill the form as required. You can remove any model from the pipeline by clicking on the \u201c Delete \u201d button. The \u201c Freeze \u201d button is useful to disable the card to prevent accidental clicks and changes. More models can be added to the pipeline by clicking \u201c + Add Model Pipeline \u201d as before and filling the forms.","title":"ADDING MODEL PIPELINE"},{"location":"lite-dashboard/#providing-the-test-image","text":"After you have added all the pipeline models properly, you can provide a test image which is used by the models and classifiers. You can choose the file using the field called \u201c Provide the Test Image \u201d. Choose an appropriate image file for testing.","title":"PROVIDING THE TEST IMAGE"},{"location":"lite-dashboard/#testing-the-image","text":"Now, the \u201c \u279c Test Image \u201d button can be clicked to test the image using the provided pipeline of models. Any validation errors will be displayed properly with error messages and which model caused it. Note : When you have already chosen a file or image in the web application and later modify it (locally) and try to run the test, it might not pass in chromium browsers (due to permissions issue). The application in that case will show \u201cFile has been modified\u201d error and you need to choose the file again using the picker. If all validation passes, it will start the testing process and on success show the results for each model. Click on the \u201c View Test Result \u25bd \u201d button to toggle its result. It might contain an error message in red font color if the model did not run successfully. Else, it will show the result properly as required. The pre-processor will show the processed image. The post-processor and classifier will show its JSON response. Similarly, Watson classifier and object detection will show its own response along with best result and score. This image shows the test result of the Pre-Processor This image shows the test result of the Watson Classifier Results of the whole pipeline can be seen on the bottom of the page under the title of \u201c Pipeline Result \u201d. You can always modify the model, change the file or image and run the test again without reloading the page. Any unhandled errors might be found in Django Logs.","title":"TESTING THE IMAGE"},{"location":"lite-dashboard/#testing-with-mobile-app","text":"Images can be tested either via web application or can also be tested with ISAC-SIMO Lite Mobile App. With the app, you need to provide the Local IP address of your running Django Project. To make the project accessible from local devices, you need to run the server using following command instead: python manage.py runserver 0.0.0.0:8000 --noreload Find your IP address with ipconfig or ifconfig or similar commands and enter that IP address in the mobile app. Then, continue with the Mobile Application by uploading or capturing images of different object types as required. The images will be sent to the running server in that IP address and will pass through all the pipeline models. Make sure that, when you make changes in your model files, web app UI etc. you first need to test the image from the web app, so that the changes get stored in the application for the mobile app to use. Unless you reload the page, you can access the latest mobile api response (i.e. pipeline results) from the web app itself. After you have tested the image from the mobile app, you can click on the \" View Mobile Test Result \" button. This will load the pipeline result of the latest test image and overrides the test results content. If you need, the pipeline configuration is stored in the /static/temp/config.json file. The mobile test result (i.e. pipeline result) is stored in /static/temp/external_response.json file. All the processors, classifiers, test images are stored in the /static/temp folder.","title":"TESTING WITH MOBILE APP"},{"location":"lite-dashboard/#links","text":"ISAC-SIMO Documentation (Latest) ISAC-SIMO Backend Developer Guide ISAC-SIMO Lite Dashboard GitHub Repository ISAC-SIMO Main GitHub Repository Download PDF","title":"LINKS"},{"location":"mobile-api-guide/","text":"Mobile APIs For Mobile API documentation, we have created Postman Collection. It can be used to create Mobile Applications or can also be used in token authenticated web applications. It contains APIs for Login, Register, Profile, Image/Video Testing, Crowdsource Upload, Projects and Object Types. View Postman API Documentation Here Create a Copy on Postman Desktop","title":"Mobile API Guide"},{"location":"mobile-api-guide/#mobile-apis","text":"For Mobile API documentation, we have created Postman Collection. It can be used to create Mobile Applications or can also be used in token authenticated web applications. It contains APIs for Login, Register, Profile, Image/Video Testing, Crowdsource Upload, Projects and Object Types. View Postman API Documentation Here Create a Copy on Postman Desktop","title":"Mobile APIs"},{"location":"mobile-application/","text":"Mobile Application Introduction The Mobile application allows users to login, register and take or upload Images of different Objects (Wall, Rebar etc.) to classify it as GO or NOGO . Guest Users who choose not to login will only have access to Global Guest Project and its Object Types (If they exist). Otherwise, logged in users will only have access to Projects and the Object Types they are linked to. Login & Register Login . Register . Profile The Profile tab shows the logged in users Name, Email, Profile Image and option to Logout. Guest users will be considered a temporary Anonymous User. Home The Home Tab is the Mobile Dashboard. There are Information, Report and Quality Check Options that can be clicked. Quality Check is the main option where users can test images of different objects and view the result. Quality Check As mentioned multiple times above, Guest Users who choose not to login will only have access to Global Guest Project and its Object Types (If they exist). While, logged in users will only have access to Projects and the Object Types they are linked to. This Quality Checks page lists all the Object Type users can test by taking photographs. First, Choose the Object Type for which to perform a quality test. In the image below, users may choose Facade Wall, Rebar Shapes etc. The checks list might vary depending on users GPS location. For this guide, let's choose Rebar Shape. Clicking on Rebar Shape in the list will open another page that shows Instruction and a Sample Image on how to properly capture the Image. Following the instruction will make sure that the result will be better and accurate. Next, you can click on the \u201cLet\u2019s Go\u201d button to capture or upload images for testing. The Uploaded image will be passed through all Classifiers/Models Pipeline that is linked to the chosen Object Type. Before uploading users can crop & edit the images. Instruction Upload Make sure to Crop the Image so that the Object is centred and no other obstruction is visible. The image brightness, contrast, hue etc. can be easily adjusted in the next screen. Edit Image Submit Image After clicking the \u201cSubmit\u201d button the Image is sent and processed by the server. And the result and score is shown. Results can generally be Go, Nogo or No Result and score are generally 0 to 100 percent. Score is the confidence level on the result. For Example, if the result is \u201cGO\u201d and the score is 90% like below, then it suggests that the Image passed with 90% confidence. (i.e. the Image is Okay and in this case Rebar Shape is good) The Score and Result of all Pipeline is accessible in the Web Dashboard. Here, all Pipeline Classifiers and the score they returned are shown.","title":"Mobile Application"},{"location":"mobile-application/#mobile-application-introduction","text":"The Mobile application allows users to login, register and take or upload Images of different Objects (Wall, Rebar etc.) to classify it as GO or NOGO . Guest Users who choose not to login will only have access to Global Guest Project and its Object Types (If they exist). Otherwise, logged in users will only have access to Projects and the Object Types they are linked to.","title":"Mobile Application Introduction"},{"location":"mobile-application/#login-register","text":"Login . Register .","title":"Login &amp; Register"},{"location":"mobile-application/#profile","text":"The Profile tab shows the logged in users Name, Email, Profile Image and option to Logout. Guest users will be considered a temporary Anonymous User.","title":"Profile"},{"location":"mobile-application/#home","text":"The Home Tab is the Mobile Dashboard. There are Information, Report and Quality Check Options that can be clicked. Quality Check is the main option where users can test images of different objects and view the result.","title":"Home"},{"location":"mobile-application/#quality-check","text":"As mentioned multiple times above, Guest Users who choose not to login will only have access to Global Guest Project and its Object Types (If they exist). While, logged in users will only have access to Projects and the Object Types they are linked to. This Quality Checks page lists all the Object Type users can test by taking photographs. First, Choose the Object Type for which to perform a quality test. In the image below, users may choose Facade Wall, Rebar Shapes etc. The checks list might vary depending on users GPS location. For this guide, let's choose Rebar Shape. Clicking on Rebar Shape in the list will open another page that shows Instruction and a Sample Image on how to properly capture the Image. Following the instruction will make sure that the result will be better and accurate. Next, you can click on the \u201cLet\u2019s Go\u201d button to capture or upload images for testing. The Uploaded image will be passed through all Classifiers/Models Pipeline that is linked to the chosen Object Type. Before uploading users can crop & edit the images. Instruction Upload Make sure to Crop the Image so that the Object is centred and no other obstruction is visible. The image brightness, contrast, hue etc. can be easily adjusted in the next screen. Edit Image Submit Image After clicking the \u201cSubmit\u201d button the Image is sent and processed by the server. And the result and score is shown. Results can generally be Go, Nogo or No Result and score are generally 0 to 100 percent. Score is the confidence level on the result. For Example, if the result is \u201cGO\u201d and the score is 90% like below, then it suggests that the Image passed with 90% confidence. (i.e. the Image is Okay and in this case Rebar Shape is good) The Score and Result of all Pipeline is accessible in the Web Dashboard. Here, all Pipeline Classifiers and the score they returned are shown.","title":"Quality Check"},{"location":"web-application/","text":"Web Application Introduction This section provides a brief Guide on using the ISAC-SIMO Web Application. It guides you through user registration, project management, classifier/model creation, object types and more. Login & Register \u25b6\ufe0f Watch Video Login Register Users can easily register themselves, and choose to be either a normal User or a Project Admin . As the name suggests, Project Admin can create or moderate Projects, Users, Models and Pipelines. While, normal users can test images and manage their own tested images. After logging into the web application, the user is presented with a dashboard. Users can click on their name in the sidebar and open the profile page, where they can update profile information and generate API Access Tokens. Dashboard Projects Permissions : Admin (All), Project Admin (Own) Example : Colombia Project, Bisaya Region Rebar Quality Verify Project etc. Update: IBM Watson API not requires IBM Service URL . This field needs to be filled appropriately, and the value can be found at watson dashboard. Create Project Users can choose to add Watson Object Detect Models Name and the IBM Watson API Key while creating the Project. Or, they can also link a Local/Offline Detect Model. Then, while testing images, users can choose the Project to test on. First the Image will be passed via provided Object Detect Model to find out the possible object in the Image (instead of manually defining the Object Type while testing). If Marked as \u201cGlobal\u201d it will be shown to Guest/Offline Users in Mobile App. Logged in users will be shown via linked projects. View Projects Test Projects, Object Detection Model Object Types Permissions : Admin (All), Project Admin (Own & Linked) Example : wall, rebar, rebar shapes etc. Create/Add Object Types The Name of Object Type must be unique for that specific Project. \u26a0 If an Object Type is linked to a Project (by Admin), then the Project Admin will have full Access to it . View Object Types Admin and Project Admin can manage Object Types, Change Order of Classifier Pipelines, Test Images against this Object Type and more. Not shown in the picture below: Admin can also choose multiple countries in which certain checks should be available. Depending on mobile app users GPS location, it will show or hide certain object type from check list. Models / Classifier Permissions : Admin (All), Project Admin (Own & Linked) Type : IBM Watson Train New Model IBM Watson add Pre-Trained Model Offline Model (Classifier, Pre/Post Processor) Update: IBM Watson API not requires IBM Service URL . This field needs to be filled appropriately, and the value can be found at watson dashboard. Create Models Admin or Project Admin can add a new Model. A new model can be trained by uploading zipped images and choosing to process it or not. Users can also add Pre-Trained Model by specifying the Classifier Name and IBM Watson API Key. Also, users can add and link Offline Model / Script that can be used either as a Classifier or Pre/Post Processor. View Models Users can easily view all the Models/Classifiers for each Project and Object Types. The Offline Model, Labels, Pre/Post Processor Status is easily viewable. Users can also test this Specific Model for debug purposes. As shown above in the Object Types section, Users can also easily change the Order of Classifier in the pipeline by simple drag and drop Test Model Offline Models Permissions : Admin (All), Project Admin (Only Own) Type : Pre-Processor (Python 3 Format, Useful to Process Image e.g. Gaussian Blur/Resize image etc.) Post-Processor (Python 3 Format, Alter/Calculate: Result & Score or do custom classification) Classifier (h5, keras, py format which should classify an image and return scores appropriately) Object Detect (h5, keras, py format which should return detected objects score and bound area) As we saw in the Models/Classifiers section above, Users can link custom Offline Models to any Classifier. The Offline Model can be of type Processor (Pre/Post), Classifier and Object Detect. A Classifier can only link Processor or Classifier, while Object Detect can be linked to a Project. The Response and data receivable by Offline Model is predefined and should follow strict guidelines. Technical Details on creating Offline Model can be found here . https://www.isac-simo.net/app/offline_model/readme.md Add Offline Model View Offline Models Users can manage Offline Models and update the model file. If the offline model is python 3 format, users can also check the dependencies used by it. Admin can if required install these dependencies via terminal. And, just like Model and Offline Model can also be tested. Test Offline Models This example shows a quick test of a Preprocessor Offline Model. The Preprocessor returns a processed image. Similarly, Postprocessor, Classifier and Object Detection will return certain JSON responses. File Upload Permissions : Admin (All) Admin Users can Upload other types of Files, Images, Helper Models etc using this file upload feature. After uploading the file, the user can copy the root path to that file, which can be used inside of the pre/post processor, classifiers etc. Also, users can share the file as normal web url. \u26a0 Other users can use the root path inside offline models, but will not have file upload access. Upload File View Files Users Permissions : Admin (All), Project Admin (View/Edit Lower Level User Only) Add User Admin or Project Admin can create/edit or register users themselves. Admin can assign the user to any Project, while Project Admin can assign users to their own Projects only. Project Admin will only have view and edit access to the users (But not Admin Users). \u26a0 Note that Newly registered Project Admin must be Verified by Admin before they login. View Users Here, Admin will have full control over all users. But, Project Admin can edit and assign users (Non admin) to their projects; but cannot delete the users. Images Add Images Images can be tested with Mobile Applications or APIs. But, Admin also has Dashboard access to Add and Test Images. Any Image can be tested by choosing a Project or an Object Type . If the Project is chosen then the Object Detection Model linked in the Project will be used to detect the Object Type and is passed through the Classifier Pipeline. Similarly, if Object Type is chosen then this is passed through the Classifiers in this object type (without caring about the project). \u26a0 You Can Choose an Object Type to Force and use that type (Or Else object detect model from chosen Project is used to detect the object in the given image) View Images Admin can View and Manage all Images. Project Admin can manage images linked to their Projects only. Government & Engineer can view linked Project Images. Normal Users can only Manage their own Images. Here, in the Image View page a quick preview of Image, and its result and score can be viewed by clicking over the number list . The Border of Number list suggests; Green is Go , Red is No Go and Orange is No Result . View Image Test Result Inside the Update Image page, you can see Test Results at the bottom. Clicking on Images will Popup the Image and show brief Result, Score and Object Detected. By clicking the Info/Review icon, we can see detailed score and result for each Pipeline along with the ability to verify the result. Test Result Test Form Terminal Permissions : Admin (All) Admin has the ability to run some commands like; install python packages, list packages installed etc. If an offline classifier or processor needs a specific package or library, then Admin can install it here. The Terminal Output is visible and all commands are sanitized properly. Translator Permissions : Admin (All) Only Admin has the ability to view and edit the translator. Translator can be access from the dashboard sidebar. Language can be toggle from the navbar. Languages list and toggler Trasnlator Form Miscellaneous Right Sidebar Permissions : Admin (All), Project Admin (Dump JSON Image Data) IBM Watson & More Permissions : Admin (All), Project Admin (Linked) Feature Includes; Retraining Classifiers, Fetch Classifier Details & Training Status, Fetch Object Type Details & Status and View Offline Model Details. Example Response for Fetch Classifier Details: Crowdsource Image \u25b6\ufe0f Watch Video Permissions : Admin (All), Project Admin (All), User (Own) The Crowdsource Feature allows anyone to upload multiple images of different Object Types. These uploaded Images can be used while Training different Models by the Admin or Project Admin. The User who uploaded the images, must confirm that they are willing to transfer the Copyright to ISAC-SIMO & agree that it can be used and shared freely. Uploading new Contribution from Backend . View / Manage Contribution Images . Images can be uploaded from the ISAC-SIMO Mobile Application. Similarly, any application can implement the API to integrate crowdsource functionality. Image Share Permissions : Admin (All), Project Admin (Own), User (Own) The Image Share/Request Feature allows anyone to request image access of certain object type to Admin. The Admin can either accept or decline the image request and provide remarks if required. When the request is accepted, users who requested it are then able to download a JSON file which contains list (array) of image urls. These image url are public and can be used to download the images directly or through python scripts or notebooks. Requesting Image of chosen object type . View / Manage Image Requests . Image Share when accepted allows users to download the JSON file for 30 days, then it is expired. After 60 days, the old expired image requests are purged. The Downloaded JSON file sample is as below: [ { \"key\": \"<unique_identifer>\", \"url\": \"https://isac-simo-storage.s3.us-east.cloud-object-storage.appdomain.cloud/<folder>/<image_name>\" }, ... ... ] From this page, users can also open Google Colab Notebook and create a clone of sample notebook created by ISAC-SIMO. Public Projects Permissions : Admin (All), Project Admin (Own), User (View/Join/Contribute) This feature allows Admin & Project Admin to easily upgrade and share Projects publicly. To share the Project, and make it available to see and contribute by other users, it must be marked as Public (via the Project Create/Edit form) After the Project has been marked as Public, it will immediately be visible in the Public Projects Page and users can view the Project Details and Join the Project themself. The Public Projects can be searched by Name, Description & Linked Object Types. Users can filter to show only the Joined Projects as well. It also conveniently shows the option to Join/Leave any Project. After the user Joins the Project, they will see the object types from this project in the list on the mobile application. By clicking on the \u201cView More\u201d button, users can view the Object Types linked to this project as well as the Classifier Pipeline used to test the objects. They can view the Classifier Information and even download the Offline Model if it exists. It also shows if the Classifier is Watson Model or not, as well as if it is Post-Processor, Pre-Processor, Detect Model, Watson Classifier or Local Classifier. The Search option allows you to filter by query. Here, if Admin or Project Admin has marked any Object Type as \u201cWishlist\u201d then the Contribute button will be visible. To Mark Object Type as Wishlist it can be done from Object Types Page and clicking on the little \u201cAdd to Wishlist\u201d (Bookmark Icon) button. \u26a0 Note that, this can only be done if the Project it is linked to is marked as Public To Contribute, as mentioned above, users can click the Contribute button (Also available inside View Contribution Page). To Contribute, the form should be filled with Title, Description explaining the contribution itself. And, can also include one file (zipped multiple files) with required documents and models. After the Contribution has been submitted, the Admin or Project Admin of that particular Project can review the contribution and can mark it as \u201cHelpful\u201d. Other users can then view all Helpful Contributions along with their own. (And Edit or Delete their Own Contribution if required) Admin or Project Admin can also temporarily unmark Project as Public to stop receiving contributions. But, they will still be able to manage old contributions. (Unless removed from the Project) The Contribution Form . View Contributions . Mark Contribution as helpful (By Admin or Project Admin) .","title":"Web Application"},{"location":"web-application/#web-application-introduction","text":"This section provides a brief Guide on using the ISAC-SIMO Web Application. It guides you through user registration, project management, classifier/model creation, object types and more.","title":"Web Application Introduction"},{"location":"web-application/#login-register","text":"\u25b6\ufe0f Watch Video Login Register Users can easily register themselves, and choose to be either a normal User or a Project Admin . As the name suggests, Project Admin can create or moderate Projects, Users, Models and Pipelines. While, normal users can test images and manage their own tested images. After logging into the web application, the user is presented with a dashboard. Users can click on their name in the sidebar and open the profile page, where they can update profile information and generate API Access Tokens.","title":"Login &amp; Register"},{"location":"web-application/#dashboard","text":"","title":"Dashboard"},{"location":"web-application/#projects","text":"Permissions : Admin (All), Project Admin (Own) Example : Colombia Project, Bisaya Region Rebar Quality Verify Project etc. Update: IBM Watson API not requires IBM Service URL . This field needs to be filled appropriately, and the value can be found at watson dashboard.","title":"Projects"},{"location":"web-application/#create-project","text":"Users can choose to add Watson Object Detect Models Name and the IBM Watson API Key while creating the Project. Or, they can also link a Local/Offline Detect Model. Then, while testing images, users can choose the Project to test on. First the Image will be passed via provided Object Detect Model to find out the possible object in the Image (instead of manually defining the Object Type while testing). If Marked as \u201cGlobal\u201d it will be shown to Guest/Offline Users in Mobile App. Logged in users will be shown via linked projects.","title":"Create Project"},{"location":"web-application/#view-projects","text":"","title":"View Projects"},{"location":"web-application/#test-projects-object-detection-model","text":"","title":"Test Projects, Object Detection Model"},{"location":"web-application/#object-types","text":"Permissions : Admin (All), Project Admin (Own & Linked) Example : wall, rebar, rebar shapes etc.","title":"Object Types"},{"location":"web-application/#createadd-object-types","text":"The Name of Object Type must be unique for that specific Project. \u26a0 If an Object Type is linked to a Project (by Admin), then the Project Admin will have full Access to it .","title":"Create/Add Object Types"},{"location":"web-application/#view-object-types","text":"Admin and Project Admin can manage Object Types, Change Order of Classifier Pipelines, Test Images against this Object Type and more. Not shown in the picture below: Admin can also choose multiple countries in which certain checks should be available. Depending on mobile app users GPS location, it will show or hide certain object type from check list.","title":"View Object Types"},{"location":"web-application/#models-classifier","text":"Permissions : Admin (All), Project Admin (Own & Linked) Type : IBM Watson Train New Model IBM Watson add Pre-Trained Model Offline Model (Classifier, Pre/Post Processor) Update: IBM Watson API not requires IBM Service URL . This field needs to be filled appropriately, and the value can be found at watson dashboard.","title":"Models / Classifier"},{"location":"web-application/#create-models","text":"Admin or Project Admin can add a new Model. A new model can be trained by uploading zipped images and choosing to process it or not. Users can also add Pre-Trained Model by specifying the Classifier Name and IBM Watson API Key. Also, users can add and link Offline Model / Script that can be used either as a Classifier or Pre/Post Processor.","title":"Create Models"},{"location":"web-application/#view-models","text":"Users can easily view all the Models/Classifiers for each Project and Object Types. The Offline Model, Labels, Pre/Post Processor Status is easily viewable. Users can also test this Specific Model for debug purposes. As shown above in the Object Types section, Users can also easily change the Order of Classifier in the pipeline by simple drag and drop","title":"View Models"},{"location":"web-application/#test-model","text":"","title":"Test Model"},{"location":"web-application/#offline-models","text":"Permissions : Admin (All), Project Admin (Only Own) Type : Pre-Processor (Python 3 Format, Useful to Process Image e.g. Gaussian Blur/Resize image etc.) Post-Processor (Python 3 Format, Alter/Calculate: Result & Score or do custom classification) Classifier (h5, keras, py format which should classify an image and return scores appropriately) Object Detect (h5, keras, py format which should return detected objects score and bound area) As we saw in the Models/Classifiers section above, Users can link custom Offline Models to any Classifier. The Offline Model can be of type Processor (Pre/Post), Classifier and Object Detect. A Classifier can only link Processor or Classifier, while Object Detect can be linked to a Project. The Response and data receivable by Offline Model is predefined and should follow strict guidelines. Technical Details on creating Offline Model can be found here . https://www.isac-simo.net/app/offline_model/readme.md","title":"Offline Models"},{"location":"web-application/#add-offline-model","text":"","title":"Add Offline Model"},{"location":"web-application/#view-offline-models","text":"Users can manage Offline Models and update the model file. If the offline model is python 3 format, users can also check the dependencies used by it. Admin can if required install these dependencies via terminal. And, just like Model and Offline Model can also be tested.","title":"View Offline Models"},{"location":"web-application/#test-offline-models","text":"This example shows a quick test of a Preprocessor Offline Model. The Preprocessor returns a processed image. Similarly, Postprocessor, Classifier and Object Detection will return certain JSON responses.","title":"Test Offline Models"},{"location":"web-application/#file-upload","text":"Permissions : Admin (All) Admin Users can Upload other types of Files, Images, Helper Models etc using this file upload feature. After uploading the file, the user can copy the root path to that file, which can be used inside of the pre/post processor, classifiers etc. Also, users can share the file as normal web url. \u26a0 Other users can use the root path inside offline models, but will not have file upload access.","title":"File Upload"},{"location":"web-application/#upload-file","text":"","title":"Upload File"},{"location":"web-application/#view-files","text":"","title":"View Files"},{"location":"web-application/#users","text":"Permissions : Admin (All), Project Admin (View/Edit Lower Level User Only)","title":"Users"},{"location":"web-application/#add-user","text":"Admin or Project Admin can create/edit or register users themselves. Admin can assign the user to any Project, while Project Admin can assign users to their own Projects only. Project Admin will only have view and edit access to the users (But not Admin Users). \u26a0 Note that Newly registered Project Admin must be Verified by Admin before they login.","title":"Add User"},{"location":"web-application/#view-users","text":"Here, Admin will have full control over all users. But, Project Admin can edit and assign users (Non admin) to their projects; but cannot delete the users.","title":"View Users"},{"location":"web-application/#images","text":"","title":"Images"},{"location":"web-application/#add-images","text":"Images can be tested with Mobile Applications or APIs. But, Admin also has Dashboard access to Add and Test Images. Any Image can be tested by choosing a Project or an Object Type . If the Project is chosen then the Object Detection Model linked in the Project will be used to detect the Object Type and is passed through the Classifier Pipeline. Similarly, if Object Type is chosen then this is passed through the Classifiers in this object type (without caring about the project). \u26a0 You Can Choose an Object Type to Force and use that type (Or Else object detect model from chosen Project is used to detect the object in the given image)","title":"Add Images"},{"location":"web-application/#view-images","text":"Admin can View and Manage all Images. Project Admin can manage images linked to their Projects only. Government & Engineer can view linked Project Images. Normal Users can only Manage their own Images. Here, in the Image View page a quick preview of Image, and its result and score can be viewed by clicking over the number list . The Border of Number list suggests; Green is Go , Red is No Go and Orange is No Result .","title":"View Images"},{"location":"web-application/#view-image-test-result","text":"Inside the Update Image page, you can see Test Results at the bottom. Clicking on Images will Popup the Image and show brief Result, Score and Object Detected. By clicking the Info/Review icon, we can see detailed score and result for each Pipeline along with the ability to verify the result. Test Result Test Form","title":"View Image Test Result"},{"location":"web-application/#terminal","text":"Permissions : Admin (All) Admin has the ability to run some commands like; install python packages, list packages installed etc. If an offline classifier or processor needs a specific package or library, then Admin can install it here. The Terminal Output is visible and all commands are sanitized properly.","title":"Terminal"},{"location":"web-application/#translator","text":"Permissions : Admin (All) Only Admin has the ability to view and edit the translator. Translator can be access from the dashboard sidebar. Language can be toggle from the navbar. Languages list and toggler Trasnlator Form","title":"Translator"},{"location":"web-application/#miscellaneous","text":"","title":"Miscellaneous"},{"location":"web-application/#right-sidebar","text":"Permissions : Admin (All), Project Admin (Dump JSON Image Data)","title":"Right Sidebar"},{"location":"web-application/#ibm-watson-more","text":"Permissions : Admin (All), Project Admin (Linked) Feature Includes; Retraining Classifiers, Fetch Classifier Details & Training Status, Fetch Object Type Details & Status and View Offline Model Details. Example Response for Fetch Classifier Details:","title":"IBM Watson &amp; More"},{"location":"web-application/#crowdsource-image","text":"\u25b6\ufe0f Watch Video Permissions : Admin (All), Project Admin (All), User (Own) The Crowdsource Feature allows anyone to upload multiple images of different Object Types. These uploaded Images can be used while Training different Models by the Admin or Project Admin. The User who uploaded the images, must confirm that they are willing to transfer the Copyright to ISAC-SIMO & agree that it can be used and shared freely. Uploading new Contribution from Backend . View / Manage Contribution Images . Images can be uploaded from the ISAC-SIMO Mobile Application. Similarly, any application can implement the API to integrate crowdsource functionality.","title":"Crowdsource Image"},{"location":"web-application/#image-share","text":"Permissions : Admin (All), Project Admin (Own), User (Own) The Image Share/Request Feature allows anyone to request image access of certain object type to Admin. The Admin can either accept or decline the image request and provide remarks if required. When the request is accepted, users who requested it are then able to download a JSON file which contains list (array) of image urls. These image url are public and can be used to download the images directly or through python scripts or notebooks. Requesting Image of chosen object type . View / Manage Image Requests . Image Share when accepted allows users to download the JSON file for 30 days, then it is expired. After 60 days, the old expired image requests are purged. The Downloaded JSON file sample is as below: [ { \"key\": \"<unique_identifer>\", \"url\": \"https://isac-simo-storage.s3.us-east.cloud-object-storage.appdomain.cloud/<folder>/<image_name>\" }, ... ... ] From this page, users can also open Google Colab Notebook and create a clone of sample notebook created by ISAC-SIMO.","title":"Image Share"},{"location":"web-application/#public-projects","text":"Permissions : Admin (All), Project Admin (Own), User (View/Join/Contribute) This feature allows Admin & Project Admin to easily upgrade and share Projects publicly. To share the Project, and make it available to see and contribute by other users, it must be marked as Public (via the Project Create/Edit form) After the Project has been marked as Public, it will immediately be visible in the Public Projects Page and users can view the Project Details and Join the Project themself. The Public Projects can be searched by Name, Description & Linked Object Types. Users can filter to show only the Joined Projects as well. It also conveniently shows the option to Join/Leave any Project. After the user Joins the Project, they will see the object types from this project in the list on the mobile application. By clicking on the \u201cView More\u201d button, users can view the Object Types linked to this project as well as the Classifier Pipeline used to test the objects. They can view the Classifier Information and even download the Offline Model if it exists. It also shows if the Classifier is Watson Model or not, as well as if it is Post-Processor, Pre-Processor, Detect Model, Watson Classifier or Local Classifier. The Search option allows you to filter by query. Here, if Admin or Project Admin has marked any Object Type as \u201cWishlist\u201d then the Contribute button will be visible. To Mark Object Type as Wishlist it can be done from Object Types Page and clicking on the little \u201cAdd to Wishlist\u201d (Bookmark Icon) button. \u26a0 Note that, this can only be done if the Project it is linked to is marked as Public To Contribute, as mentioned above, users can click the Contribute button (Also available inside View Contribution Page). To Contribute, the form should be filled with Title, Description explaining the contribution itself. And, can also include one file (zipped multiple files) with required documents and models. After the Contribution has been submitted, the Admin or Project Admin of that particular Project can review the contribution and can mark it as \u201cHelpful\u201d. Other users can then view all Helpful Contributions along with their own. (And Edit or Delete their Own Contribution if required) Admin or Project Admin can also temporarily unmark Project as Public to stop receiving contributions. But, they will still be able to manage old contributions. (Unless removed from the Project) The Contribution Form . View Contributions . Mark Contribution as helpful (By Admin or Project Admin) .","title":"Public Projects"}]}